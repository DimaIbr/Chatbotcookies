{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd59ded1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd59ded1",
        "outputId": "bf9a9d78-d780-4aca-b39f-dae1c62361ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.2 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 596 kB 9.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.6 MB 7.5 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.6.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 84 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.6.0 pyyaml-6.0 tokenizers-0.12.1 transformers-4.19.2\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip3 install transformers\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)\n",
        "from typing import Tuple, List\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import (AutoTokenizer, TextDataset, DataCollatorForLanguageModeling,\n",
        "                          AutoModelForCausalLM,\n",
        "                          Trainer, TrainingArguments,\n",
        "                          TextGenerationPipeline, pipeline)\n",
        "from torch.cuda import empty_cache\n",
        "import os\n",
        "\n",
        "input_path       : str = \"/content/drive/MyDrive/datatrue\"\n",
        "train_path       : str = \"./train.txt\"\n",
        "test_path        : str = \"./test.txt\"\n",
        "output_dir       : str = \"./output\" \n",
        "split_token      : str = \"<|endoftext|>\"\n",
        "model_name       : str = \"sberbank-ai/sbert_large_nlu_ru\"\n",
        "input_block_size : int = 128\n",
        "train_batch_size : int = 6\n",
        "eval_batch_size  : int = 16\n",
        "epochs_count     : int = 5\n",
        "eval_every       : int = 500\n",
        "save_every       : int = 2500\n",
        "warmup_steps     : int = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40bf5872",
      "metadata": {
        "id": "40bf5872"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_test_train_text() -> Tuple[List[str], List[str]]:\n",
        "    data = []\n",
        "    for filename in os.listdir(input_path):\n",
        "        f = os.path.join(input_path, filename)\n",
        "        if os.path.isfile(f):\n",
        "            with open(f, \"r\", encoding=\"utf-8\") as file:\n",
        "                data.append(file.read())\n",
        "        \n",
        "    return train_test_split(data, test_size=0.2)\n",
        "\n",
        "def save_text_file(text : List[str], path : str):\n",
        "    data = split_token.join(text)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
        "        file.write(data)\n",
        "    \n",
        "train_data, test_data = get_test_train_text()\n",
        "save_text_file(train_data, train_path)\n",
        "save_text_file(test_data, test_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b710c19",
      "metadata": {
        "id": "6b710c19"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_dataset(tokenizer : AutoTokenizer) -> Tuple[TextDataset, TextDataset, DataCollatorForLanguageModeling]:\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, \n",
        "                                file_path=train_path, \n",
        "                                block_size=input_block_size)\n",
        "    test_dataset = TextDataset(tokenizer=tokenizer, \n",
        "                               file_path=test_path, \n",
        "                               block_size=input_block_size)\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,\n",
        "                                                    mlm=False)\n",
        "    return train_dataset, test_dataset, data_collator\n",
        "\n",
        "def load_model(path : str = model_name, resize : bool = True) -> Tuple[AutoTokenizer, AutoModelForCausalLM]:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, eos_token=split_token, pad_token=split_token)\n",
        "    model = AutoModelForCausalLM.from_pretrained(path)\n",
        "    if resize:\n",
        "        model.resize_token_embeddings(len(tokenizer))\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54d031e4",
      "metadata": {
        "scrolled": true,
        "id": "54d031e4",
        "outputId": "24822c53-2dcd-4fac-ec2f-0daac0963895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
            "Some weights of BertLMHeadModel were not initialized from the model checkpoint at sberbank-ai/sbert_large_nlu_ru and are newly initialized: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 13830\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 11525\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='11525' max='11525' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [11525/11525 6:08:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.557900</td>\n",
              "      <td>0.125530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.086400</td>\n",
              "      <td>0.047307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.042700</td>\n",
              "      <td>0.027781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.025900</td>\n",
              "      <td>0.017750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.013800</td>\n",
              "      <td>0.012883</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.005800</td>\n",
              "      <td>0.011862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.011022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>0.010529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.004300</td>\n",
              "      <td>0.009160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.008855</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>0.008356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.002000</td>\n",
              "      <td>0.008574</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.007799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>0.008155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.007614</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.007698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.007776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.007833</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.007812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.007653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.007627</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.007673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.007663</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./output/checkpoint-2500\n",
            "Configuration saved in ./output/checkpoint-2500/config.json\n",
            "Model weights saved in ./output/checkpoint-2500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./output/checkpoint-5000\n",
            "Configuration saved in ./output/checkpoint-5000/config.json\n",
            "Model weights saved in ./output/checkpoint-5000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./output/checkpoint-7500\n",
            "Configuration saved in ./output/checkpoint-7500/config.json\n",
            "Model weights saved in ./output/checkpoint-7500/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to ./output/checkpoint-10000\n",
            "Configuration saved in ./output/checkpoint-10000/config.json\n",
            "Model weights saved in ./output/checkpoint-10000/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 3529\n",
            "  Batch size = 16\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def create_metrics_computer():\n",
        "    metrics = {\n",
        "        \"bleu\" : load_metric(\"bleu\"),\n",
        "        \"rouge\" : load_metric(\"rouge\")\n",
        "    }\n",
        "        \n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        bleu_results = metrics[\"bleu\"].compute(predictions=predictions, references=labels)\n",
        "        rouge_results = metrics[\"rouge\"].compute(predictions=predictions, references=labels)\n",
        "        metrics_results= {\n",
        "            \"bleu\" : bleu_results[\"bleu\"],\n",
        "            \"rouge1\" : rouge_results[\"rouge1\"].mid.fmeasure,\n",
        "            \"rouge2\" : rouge_results[\"rouge2\"].mid.fmeasure,\n",
        "            \"rougeL\" : rouge_results[\"rougeL\"].mid.fmeasure,\n",
        "            \"rougeLsum\" : rouge_results[\"rougeLsum\"].mid.fmeasure,\n",
        "        }\n",
        "        return metrics_results\n",
        "    \n",
        "    return compute_metrics\n",
        "\n",
        "def create_trainer(model : AutoModelForCausalLM,\n",
        "                   data_collator : DataCollatorForLanguageModeling,\n",
        "                   train_dataset : TextDataset,\n",
        "                   test_dataset  : TextDataset) -> Trainer:\n",
        "    training_args = TrainingArguments(output_dir=output_dir,\n",
        "                                      overwrite_output_dir=True,\n",
        "                                      evaluation_strategy=\"steps\",\n",
        "                                      num_train_epochs=epochs_count,\n",
        "                                      per_device_train_batch_size=train_batch_size,\n",
        "                                      per_device_eval_batch_size=eval_batch_size,\n",
        "                                      logging_steps=eval_every,\n",
        "                                      eval_steps=eval_every,\n",
        "                                      save_steps=save_every,\n",
        "                                      warmup_steps=warmup_steps)\n",
        "    #compute_metrics = create_metrics_computer()\n",
        "    trainer = Trainer(model=model,\n",
        "                      args=training_args,\n",
        "                      data_collator=data_collator,\n",
        "                      train_dataset=train_dataset,\n",
        "                      eval_dataset=test_dataset)\n",
        "                      # –ù–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –Ω–∞ –∫–∞–∂–¥–æ–π eval_every –∏—Ç–µ—Ä–∞—Ü–∏–∏ —É –º–µ–Ω—è –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏,\n",
        "                      # —Ç–∞–∫ —á—Ç–æ –Ω–∞–¥–æ –±—É–¥–µ—Ç –ø—Ä–æ—Å—Ç–æ –≤ –∫–æ–Ω—Ü–µ –æ–¥–∏–Ω —Ä–∞–∑ –º–µ—Ç—Ä–∏–∫–∏ –≤—ã—á–∏—Å–ª–∏—Ç—å, –∞ —Ç—É—Ç –æ–±—Ö–æ–¥–∏—Ç—å—Å—è loss-–æ–º.\n",
        "                      #compute_metrics=compute_metrics)\n",
        "    return trainer\n",
        "tokenizer, model = load_model()\n",
        "train_dataset, test_dataset, data_collator = load_dataset(tokenizer)\n",
        "trainer = create_trainer(model, data_collator, train_dataset, test_dataset)\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "name": "–ö–æ–ø–∏—è –±–ª–æ–∫–Ω–æ—Ç–∞ \"–ö–æ–ø–∏—è –±–ª–æ–∫–Ω–æ—Ç–∞ \"ruGPT3.ipynb\"\"",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}